\section{Experimental Setup}\label{sect:experimental-setup}


\subsection{Datasets}\label{subsect:datasets}
The algorithms evaluated in this paper are compared on a variety of synthetic
and real-world univariate time series, taken from the \gls{nab}-collection.
In this section the dataset will be described in detail.

\subsubsection{Controversy}
Recent research inspired by~\cite{Nakamura.2020} has raised doubts about the
adequacy of multiple anomaly detection datasets~\cite{Renjie.2020}. Prominently
included are Yahoo!S5, \gls{nab}, NASA~\cite{Hundman.2018}, and OMNI~\cite{Su.2019}
datasets.

It is objected that these datasets suffer from one or multiple of the following
flaws:
\begin{description}
    \item[Triviality] A dataset is considered trivial if it can be \textit{solved}
    by a simple combination of statistical operations such as \textit{mean},
    \textit{max}, \textit{std}, \textit{diff}.
    
    Examples that demonstrate this flaw are taken mainly from the Yahoo!S5 dataset
    for which also a brute force computational example is provided. Three time series
    from OMNI are examined and only a single one from \gls{nab}.
    
    For NASA, \textcite{Renjie.2020} claim that about 90\% of included anomalies
    are trivial and show \(>10\) examples. For OMNI 50\% are claimed to be trial,
    but only three examples are given. For \gls{nab}, the authors claim that
    \textit{most} time series are trivial, but omit evidence by giving only a
    single example.

    \item[Unrealistic Density] A dataset is considered unrealistically dense, if
    \begin{enumerate*}[a.)]
        \item a large portion (e.g.\ \(> 0.1\)) of observations are labeled anomalous,
        \item many (e.g.\ \(> 5\)) proximal regions are marked as separate anomalies,
        \item separate anomalies are proximal (e.g.\ \textit{sandwiching} a single
        normal observation)~\cite[cf.][]{Renjie.2020}.
    \end{enumerate*}

    Examples that demonstrate this flaw are taken from Yahoo!S5 and NASA\@.

    \item[Mislabeled Ground Truth] A dataset is mislabeled if it contains any
    false positives or false positives.

    Examples that demonstrate this flaw are taken mainly from Yahoo!S5. A single
    time series from \gls{nab} is given examined.

    \item[Run-to-failure Bias] A dataset suffers from the run-to-failure bias if
    anomalies appear only towards its end without normal observations following
    them.

    Examples that demonstrate this flaw are taken from Yahoo!S5 and NASA\@.
\end{description}

In summary, the majority of flaws are to be found in Yahoo!S5 and NASA datasets.
While the authors choose not to provide evidence for the triviality of \gls{nab},
the most important lesson from their work applies here as well:
\textbf{Anomaly detection is a visual domain and researchers are responsible for
providing not just scores, but visual examination of their results.}

Besides, \textcite{Renjie.2020} mention the creation of a novel benchmark dataset,
but its release is still delayed. 



\subsubsection{Numenta Anomaly Benchmark}
The \acrfull{nab} was created as a compilation of 11 synthetic and 47 real
domain specific time series. Of the 11 synthetic time series, 5 do not contain
anomalous values. Besides that, every time series consists of both a time stamp
and a single numeric value. All time series were labeled according to a unified
procedure \todo{cite labeling instructions} by a team of multiple researchers.
It was released in October 2015, to
\begin{enumerate*}[a.)]
    \item react to a lack of publicly available benchmarks
    for univariate anomaly detection (although by that time the Yahoo!S5 benchmark
    had been released already) and 
    \item to showcase the results of their HTM algorithm.
\end{enumerate*}
\todo{citations!}
The upcoming paragraph describes the scoring function introduced with \gls{nab}.
This is followed (in vein of~\cite{Renjie.2020}) by a short visual examination
of the dataset.

\paragraph{Numenta Anomaly Metric}
This is the explanation of metrics.

\paragraph{Visual Examination}
This is the visual examination.


\subsubsection{Other Datasets}
0.5 pages.


% Table of several univariate datasets
\begin{table}[h]\centering
    \ra{1.3}
        \begin{tabular}{ll}
            Dataset                                                                                                                             & Evaluation    \\\midrule
            \href{https://github.com/numenta/NAB}{Numenta Anomaly Benchmark}                                                                    &               \\\addlinespace
            \href{https://www.kaggle.com/averkij/tennessee-eastman-process-simulation-dataset}{Tennessee Eastman Process Simulation Dataset}    &               \\\addlinespace
            \href{https://github.com/alan-turing-institute/TCPDBench}{Turing Change Point Benchmark}                                            &               \\\addlinespace
            \href{https://itrust.sutd.edu.sg/testbeds/secure-water-treatment-swat/}{Secure Water Treatment}                                     &               \\\addlinespace
            \href{https://en.wikipedia.org/wiki/Makridakis\_Competitions}{M-Competitions}                                                       &               \\\addlinespace
            \href{https://webscope.sandbox.yahoo.com/catalog.php?datatype=s\&did=70}{Yahoo! Webscope S5}                                        &               \\
        \end{tabular}
    \caption{Univariate Datasets}\label{tab:datasets}
\end{table}

% Table of several multivariate datasets
\begin{table}[h]\centering
    \ra{1.3}
        \begin{tabular}{ll}
            Dataset                                                                                                                             & Evaluation    \\\midrule
            \href{https://www.kaggle.com/anomalydetectionml/features}{Data from: Machine Learning-based Anomaly Detection in Software Systems}  & Rejected. Too complex.              \\\addlinespace
            \href{https://github.com/ricardovvargas/3w_dataset}{3W Dataset}                                                                     & \makecell[l]{Rejected.\\Preprocessing and dimensionality reduction were too time-consuming.}              \\\addlinespace
            \href{https://www.kaggle.com/rkuo2000/nasa-bearing-sensor-data/notebooks}{NASA Bearing Sensor Data}                                 &               \\\addlinespace
            \href{https://github.com/chickenbestlover/RNN-Time-series-Anomaly-Detection}{HOT SAX Datasets}                                      &               \\\addlinespace
            \href{https://github.com/khundman/telemanom}{Soil Moisture Active Passive (SMAP)}                                                   &               \\\addlinespace
            \href{https://github.com/khundman/telemanom}{Mars Science Laboratory  (MSL)}                                                        &               \\\addlinespace
            \href{https://www.kaggle.com/icsdataset/hai-security-dataset}{HIL-based Augmented ICS}                                              &               \\
        \end{tabular}
        \caption{Multivariate-Datasets}\label{tab:multivariate-datasets}
\end{table}
https://datasetsearch.research.google.com/search?query=time%20series%20anomaly%20detection

% Table of chosen Datasets
\begin{table}[h]\centering
    \ra{1.3}
        \begin{tabular}{ll}
            Dataset                                                                                                                             & Amount of time series'\\\midrule
            \href{https://github.com/numenta/NAB}{Numenta Anomaly Benchmark}                                                                    &  59   \\\addlinespace
            \href{https://www.kaggle.com/averkij/tennessee-eastman-process-simulation-dataset}{Tennessee Eastman Process Simulation Dataset}    &  1    \\\addlinespace
            \href{https://github.com/alan-turing-institute/TCPDBench}{Turing Change Point Benchmark}                                            &  42   \\\addlinespace
            \href{https://webscope.sandbox.yahoo.com/catalog.php?datatype=s\&did=70}{Yahoo! Webscope S5}                                        &  367  \\\addlinespace
            \href{https://www.kaggle.com/rkuo2000/nasa-bearing-sensor-data/notebooks}{NASA Bearing Sensor Data}                                 &                   \\\addlinespace
            \href{https://github.com/khundman/telemanom}{Soil Moisture Active Passive (SMAP)}                                                   &                   \\\addlinespace
            \href{https://github.com/khundman/telemanom}{Mars Science Laboratory  (MSL)}                                                        &                   \\\addlinespace
        \end{tabular}
        \caption{Chosen Datasets}\label{tab:chosen-datasets}
\end{table}



\subsection{Univariate Datasets}
\subsubsection{NAB}
\subsubsection{Turing Change Point Dataset}
``Time series were collected from various online sources including the WorldBank, EuroStat, U.S.
Census Bureau, GapMinder, and Wikipedia. A number of time series show potential change
point behavior around the financial crisis of 2007-2008, such as the price for Brent crude oil
(Figure 1), U.S. business inventories, and U.S. construction spending, as well as the GDP
series for various countries. Other time series are centered around introduced legislation, such
as the mandatory wearing of seat belts in the U.K., the Montreal Protocol regulating CFC
emissions, or the regulation of automated phone calls in the U.S. Various data sets are also
collected from existing work on change point detection and structural breaks, such as the welllog data set (O Ruanaidh and Fitzgerald Â´ , 1996), a Nile river data set (Durbin and Koopman,
2012), and a sequence from the bee-waggle data set (Oh et al., 2008). The main criterion for
including a time series was whether it displayed interesting behavior that might include an
abrupt change. Several time series were included that may not in fact contain a change point
but are nevertheless considered interesting for evaluating CPD algorithms due to features such
as seasonality or outliers.
A total of 37 real time series were collected for the change point data set. This includes 33
univariate and 4 multivariate series (with either 2 or 4 dimensions). One of the univariate time
series (uk\_coal\_employ, Figure 37 in Appendix C) has two missing observations, and nine other
series show seasonal patterns. Unbeknownst to the annotators we added 5 simulated âquality
controlâ series with known change points in the data set, which allows us to evaluate the quality
of the annotators in situ (see below). Four of these series have a change point with a shift in the
mean and the fifth has no change points. Of the quality control series with a change point two
have a change in noise distribution, one contains an outlier, and another has multiple periodic
components. The change point data set thus consists of 42 time series in total. The average
length of the time series in the dataset is 327.7, with a minimum of 15 and a maximum of 991.
See Appendix C for a complete overview of all series in the data set.''

``On average the annotators marked 7.4 unique change points per series, with a standard deviation of 7.0, a minimum of 0 and a maximum of 26.''
``some nonetheless commented on the difficulty of deciding whether a series had a
change point or not. In some cases this was due to a significant change spanning multiple
time steps, which led to ambiguity about whether the transition period should be marked as
a separate segment or not (see e.g. Figure 1). Another source of ambiguity were periodic
time series with abrupt changes (e.g. bank, Figure 6), which could be regarded as a stationary
switching distribution without abrupt changes or a series with frequent change points''

Manually annotated.





\subsection{Multivariate Datasets}

\subsubsection{NASA Bearing Sensor Data}
Relatively easy, contains four correlated time series all with non-stationarities.
I have differenced them once. Now they are non-stationary according to KPSS and
ADFuller. I have then applied PCA with one principal component to make it univariate.

\subsubsection{Telemanom}
Telemanom is significantly more complex. It is even so complex, that I am unable
to perform ADFuller on it, because of its length. However, KPSS does work and it
claims them to be non-stationary. However, differencing them once did help.
Telemanom contains 12 different time series'. Every time series with dimensionality
25 belongs to SMAP, every time series with dimensionality 55 belongs to MSL.
Every such time series contains one channel for the actual telemetry value, the
other 24 (or 54 respectively) contain boolean variables, whether a command was
sent to one of the other systems. I choose to create two datasets, one where
only the actual telemetry value is kept, the other were I stationarize the time
series and extract the first principal component via PCA. One issue is applying
PCA in this scenario is that the boolean variables are either \(\left\{0,1\right\}\),
while telemetry data is also scaled between \(\left\{ -1, 1 \right\}\). The boolean
variables are thus given equal weight which might obfuscate original telemetry
data. I therefore doubt, that the principal components will yield better results.


\subsubsection{Tennessee Eastman Process Simulation Dataset}
The dataset consists of 1000 normal and 20000 anomalous time series. The anomalous
time series' there are 20 different anomaly types. Although all series are generated
by the same system, the series are closed. There are no normal series that evolve
into anomalous once and vice versa. Further, there are no point-wise but only
series-wise labels. We count a TP if anomalies are detected in anomalous series.
And an FP if an anomaly is detected in normal series.


Training Normal: 250.000/ 500 -> 500
Training Faulty: 5.000.000/ 500 -> 10.000

Test Normal: 480.000 / 960 -> 500
Test Faulty: 9.600.000 / 960 -> 10.000

\subsubsection{Metrics}
\dots

\subsection{Algorithm overview}